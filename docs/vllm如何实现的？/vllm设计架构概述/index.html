<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-vllm如何实现的？/vllm设计架构概述" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.3">
<title data-rh="true">vllm设计架构概述 | LLM cool Docs</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://KenForever1.github.io/llm_cool_docs/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://KenForever1.github.io/llm_cool_docs/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://KenForever1.github.io/llm_cool_docs/docs/vllm如何实现的？/vllm设计架构概述"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="vllm设计架构概述 | LLM cool Docs"><meta data-rh="true" name="description" content="https://docs.vllm.ai/en/stable/design/arch_overview.html#llm-class"><meta data-rh="true" property="og:description" content="https://docs.vllm.ai/en/stable/design/arch_overview.html#llm-class"><link data-rh="true" rel="icon" href="/llm_cool_docs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://KenForever1.github.io/llm_cool_docs/docs/vllm如何实现的？/vllm设计架构概述"><link data-rh="true" rel="alternate" href="https://KenForever1.github.io/llm_cool_docs/docs/vllm如何实现的？/vllm设计架构概述" hreflang="en"><link data-rh="true" rel="alternate" href="https://KenForever1.github.io/llm_cool_docs/docs/vllm如何实现的？/vllm设计架构概述" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/llm_cool_docs/assets/css/styles.15cbc4ab.css">
<script src="/llm_cool_docs/assets/js/runtime~main.3325832d.js" defer="defer"></script>
<script src="/llm_cool_docs/assets/js/main.4378a26b.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/llm_cool_docs/"><div class="navbar__logo"><img src="/llm_cool_docs/img/logo.svg" alt="LLM cool Docs Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/llm_cool_docs/img/logo.svg" alt="LLM cool Docs Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">LLM cool Docs</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/llm_cool_docs/docs/intro">LLM大模型推理</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/llm_cool_docs/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/llm_cool_docs/docs/llama如何实现的？/gguf使用mmap进行高效读取和保存">llama如何实现的？</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/llm_cool_docs/docs/lmdeploy如何实现的？/Intervl2_8B模型的预处理preprocess有何不同">lmdeploy如何实现的？</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/llm_cool_docs/docs/vllm如何实现的？/vllm distributed_serving">vllm如何实现的？</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/llm_cool_docs/docs/vllm如何实现的？/vllm distributed_serving">vllm distributed_serving</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/llm_cool_docs/docs/vllm如何实现的？/vllm插件系统">vllm插件系统</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/llm_cool_docs/docs/vllm如何实现的？/vllm设计架构概述">vllm设计架构概述</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/llm_cool_docs/docs/vllm如何实现的？/多头注意力机制">多头注意力机制</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/llm_cool_docs/docs/vllm如何实现的？/如何利用python setuptools实现插件机制">如何利用python setuptools实现插件机制</a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/llm_cool_docs/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">vllm如何实现的？</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">vllm设计架构概述</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>vllm设计架构概述</h1></header><p><a href="https://docs.vllm.ai/en/stable/design/arch_overview.html#llm-class" target="_blank" rel="noopener noreferrer">https://docs.vllm.ai/en/stable/design/arch_overview.html#llm-class</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-系统交互方式">1 系统交互方式<a href="#1-系统交互方式" class="hash-link" aria-label="Direct link to 1 系统交互方式" title="Direct link to 1 系统交互方式">​</a></h2>
<p>vLLM提供了许多与系统交互的入口点。主要包括LLM class和OpenAI API兼容Server接口，下图显示了它们之间的关系</p>
<p><img decoding="async" loading="lazy" src="https://docs.vllm.ai/en/stable/_images/entrypoints.excalidraw.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="11-llm-class">1.1 LLM class<a href="#11-llm-class" class="hash-link" aria-label="Direct link to 1.1 LLM class" title="Direct link to 1.1 LLM class">​</a></h3>
<p>LLM Class提供了进行离线推理的主要Python接口，离线推理是在不使用单独的模型推理服务器的情况下与模型交互。</p>
<p>以下是LLM类使用示例：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from vllm import LLM, SamplingParams</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Define a list of input prompts</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">prompts = [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;Hello, my name is&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;The capital of France is&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;The largest ocean is&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Define sampling parameters</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sampling_params = SamplingParams(temperature=0.8, top_p=0.95)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Initialize the LLM engine with the OPT-125M model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llm = LLM(model=&quot;facebook/opt-125m&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Generate outputs for the input prompts</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">outputs = llm.generate(prompts, sampling_params)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Print the generated outputs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for output in outputs:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    prompt = output.prompt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    generated_text = output.outputs[0].text</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    print(f&quot;Prompt: {prompt!r}, Generated text: {generated_text!r}&quot;)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="12-兼容openai的api服务器">1.2 兼容OpenAI的API服务器<a href="#12-兼容openai的api服务器" class="hash-link" aria-label="Direct link to 1.2 兼容OpenAI的API服务器" title="Direct link to 1.2 兼容OpenAI的API服务器">​</a></h3>
<p>vLLM的第二个主要接口是通过其兼容OpenAI的API服务器。此服务器可以使用vllm-server命令启动。</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">vllm serve &lt;model&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>或者，直接使用API server模块的入口点，而不是通过vllm CLI命令。例如：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python -m vllm.entrypoints.openai.api_server --model &lt;model&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-llm-engine">2 LLM Engine<a href="#2-llm-engine" class="hash-link" aria-label="Direct link to 2 LLM Engine" title="Direct link to 2 LLM Engine">​</a></h2>
<p>LLMEngine和AsyncLLMEngine类是vLLM系统功能的核心，处理模型推理和异步请求处理。</p>
<p><img decoding="async" loading="lazy" src="https://docs.vllm.ai/en/stable/_images/llm_engine.excalidraw.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="21-llmengine-class">2.1 LLMEngine Class<a href="#21-llmengine-class" class="hash-link" aria-label="Direct link to 2.1 LLMEngine Class" title="Direct link to 2.1 LLMEngine Class">​</a></h3>
<p>LLMEngine类是vLLM发动机的核心部件。它负责接收来自客户端的请求并从模型生成输出。LLMEngine包括输入处理、模型执行（可能分布在多个主机和/或GPU上）、调度和输出处理。</p>
<ul>
<li>输入处理：使用指定的标记器处理输入文本的标记化。</li>
<li>调度：选择在每个步骤中处理哪些请求。</li>
<li>模型执行：管理语言模型的执行，包括跨多个GPU的分布式执行。</li>
<li>输出处理：处理模型生成的输出，将语言模型中的令牌ID解码为人类可读的文本。</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="22-asyncllmengine">2.2 AsyncLLMEngine<a href="#22-asyncllmengine" class="hash-link" aria-label="Direct link to 2.2 AsyncLLMEngine" title="Direct link to 2.2 AsyncLLMEngine">​</a></h3>
<p>AsyncLLMEngine类是LLMEngin类的异步包装器。它使用asyncio创建一个持续处理传入请求的后台循环。AsyncLLMEngine专为在线服务而设计，它可以处理多个并发请求并将输出流式传输到客户端。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="23-worker">2.3 Worker<a href="#23-worker" class="hash-link" aria-label="Direct link to 2.3 Worker" title="Direct link to 2.3 Worker">​</a></h3>
<p>worker是一个运行模型推理的进程。vLLM遵循使用一个进程来控制一个加速器设备（如GPU）的常见做法。例如，如果我们使用大小2的张量并行性和大小2的流水线并行性，我们总共将有4个worker。工人通过他们的级别和local_rank进行标识。rank用于全局编排，而localrank主要用于分配加速器设备和访问本地资源，如文件系统和共享内存。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="24-model-runner">2.4 Model Runner<a href="#24-model-runner" class="hash-link" aria-label="Direct link to 2.4 Model Runner" title="Direct link to 2.4 Model Runner">​</a></h3>
<p>每个worker都有一个model runner对象，负责加载和运行模型。大部分模型执行逻辑都存在于这里，例如准备输入张量和捕获柱状图。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="25-model">2.5 Model<a href="#25-model" class="hash-link" aria-label="Direct link to 2.5 Model" title="Direct link to 2.5 Model">​</a></h3>
<p>每个模型运行器对象都有一个模型对象，即实际的torch.nn。模块实例。请参阅<a href="https://docs.vllm.ai/en/stable/design/huggingface_integration.html#huggingface-integration" target="_blank" rel="noopener noreferrer">与HuggingFace的集成</a>，了解各种配置如何影响我们最终得到的类。</p>
<p>你也可以改动ModelRunner模块，比如采用Tensorrtllm去加载模型和执行推理。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-类层次结构">3 类层次结构<a href="#3-类层次结构" class="hash-link" aria-label="Direct link to 3 类层次结构" title="Direct link to 3 类层次结构">​</a></h2>
<p><img decoding="async" loading="lazy" src="https://docs.vllm.ai/en/stable/_images/hierarchy.png" alt="" class="img_ev3q"></p>
<p>这个类层次结构背后有几个重要的设计选择：</p>
<p>1.可扩展性：层次结构中的所有类都接受包含所有必要信息的配置对象。VllmConfig类是传递的主要配置对象。类层次结构相当深，每个类都需要读取它感兴趣的配置。通过将所有配置封装在一个对象中，我们可以很容易地传递配置对象并访问我们需要的配置。假设我们想添加一个只涉及模型运行器的新功能（考虑到LLM推理领域的发展速度，这种情况经常发生）。我们必须在VllmConfig类中添加一个新的配置选项。由于我们传递了整个配置对象，我们只需要将配置选项添加到VllmConfig类中，模型运行器就可以直接访问它。我们不需要更改引擎、worker或模型类的构造函数来传递新的配置选项。</p>
<p>2.统一性：模型运行器需要一个统一的接口来创建和初始化模型。vLLM支持50多种流行的开源模型。每个模型都有自己的初始化逻辑。如果构造函数签名随模型而变化，则模型运行者不知道如何相应地调用构造函数，而没有复杂且容易出错的检查逻辑。通过使模型类的构造函数统一，模型运行者可以在不知道具体模型类型的情况下轻松创建和初始化模型。这对于构建模型也很有用。视觉语言模型通常由视觉模型和语言模型组成。通过使构造函数统一，我们可以很容易地创建视觉模型和语言模型，并将它们组合成视觉语言模型。</p>
<p>注：
为了支持这一更改，所有vLLM模型的签名都已更新为：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def __init__(self, *, vllm_config: VllmConfig, prefix: str = &quot;&quot;):</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>比如如下改动方式，这样，该模型可以与vLLM的旧版本和新版本一起使用。</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">class MyOldModel(nn.Module):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        config,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        cache_config: Optional[CacheConfig] = None,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        quant_config: Optional[QuantizationConfig] = None,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        lora_config: Optional[LoRAConfig] = None,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        prefix: str = &quot;&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ) -&gt; None:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from vllm.config import VllmConfig</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class MyNewModel(MyOldModel):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self, *, vllm_config: VllmConfig, prefix: str = &quot;&quot;):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        config = vllm_config.model_config.hf_config</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        cache_config = vllm_config.cache_config</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        quant_config = vllm_config.quant_config</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        lora_config = vllm_config.lora_config</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        super().__init__(config, cache_config, quant_config, lora_config, prefix)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">if __version__ &gt;= &quot;0.6.4&quot;:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    MyModel = MyNewModel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">else:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    MyModel = MyOldModel</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>3.初始化时的分片和量化：某些特征需要更改模型权重。例如，张量并行需要分割模型权重，量化需要量化模型权重。实现此功能有两种可能的方法。一种方法是在模型初始化后更改模型权重。另一种方法是在模型初始化期间更改模型权重。vLLM选择后者。第一种方法不适用于大型模型。假设我们想用16个H100 80GB GPU运行一个405B型号（重量约为810GB）。理想情况下，每个GPU应该只加载50GB的权重。如果我们在模型初始化后更改模型权重，我们需要将整个810GB的权重加载到每个GPU，然后对权重进行分片，从而导致巨大的内存开销。相反，如果我们在模型初始化期间对权重进行分片，每一层都只会创建一个所需权重的分片，从而大大减少内存开销。同样的想法也适用于量化。请注意，我们还为模型的构造函数添加了一个额外的参数 前缀，以便模型可以根据前缀进行不同的初始化。这对于非均匀量化非常有用，其中模型的不同部分被不同地量化。前缀通常是顶级模型的空字符串，子模型的前缀是类似“视觉”或“语言”的字符串。通常，它与检查点文件中模块的状态字典的名称相匹配。</p>
<p>这种设计的一个缺点是，很难在vLLM中为单个组件编写单元测试，因为每个组件都需要由一个完整的配置对象初始化。我们通过提供一个默认初始化函数来解决这个问题，该函数创建了一个默认配置对象，其中所有字段都设置为None。如果我们想要测试的组件只关心配置对象中的几个字段，我们可以创建一个默认的配置对象并设置我们关心的字段。这样，我们就可以单独测试组件。请注意，vLLM中的许多测试都是测试整个系统的端到端测试，因此这不是一个大问题。</p>
<p>总之，完整的配置对象VllmConfig可以被视为在所有vLLM类之间共享的引擎级全局状态。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vllm如何实现的？/vllm设计架构概述.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/llm_cool_docs/docs/vllm如何实现的？/vllm插件系统"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">vllm插件系统</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/llm_cool_docs/docs/vllm如何实现的？/多头注意力机制"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">多头注意力机制</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#1-系统交互方式" class="table-of-contents__link toc-highlight">1 系统交互方式</a><ul><li><a href="#11-llm-class" class="table-of-contents__link toc-highlight">1.1 LLM class</a></li><li><a href="#12-兼容openai的api服务器" class="table-of-contents__link toc-highlight">1.2 兼容OpenAI的API服务器</a></li></ul></li><li><a href="#2-llm-engine" class="table-of-contents__link toc-highlight">2 LLM Engine</a><ul><li><a href="#21-llmengine-class" class="table-of-contents__link toc-highlight">2.1 LLMEngine Class</a></li><li><a href="#22-asyncllmengine" class="table-of-contents__link toc-highlight">2.2 AsyncLLMEngine</a></li><li><a href="#23-worker" class="table-of-contents__link toc-highlight">2.3 Worker</a></li><li><a href="#24-model-runner" class="table-of-contents__link toc-highlight">2.4 Model Runner</a></li><li><a href="#25-model" class="table-of-contents__link toc-highlight">2.5 Model</a></li></ul></li><li><a href="#3-类层次结构" class="table-of-contents__link toc-highlight">3 类层次结构</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/llm_cool_docs/docs/intro">Tutorial</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 llm_cool_docs, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>