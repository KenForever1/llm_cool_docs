"use strict";(self.webpackChunkllm_cool_docs=self.webpackChunkllm_cool_docs||[]).push([[976],{2053:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>a});const o=JSON.parse('{"id":"intro","title":"Tutorial Intro","description":"Let\'s discover LLM.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/llm_cool_docs/docs/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","next":{"title":"gguf\u4f7f\u7528mmap\u8fdb\u884c\u9ad8\u6548\u8bfb\u53d6\u548c\u4fdd\u5b58","permalink":"/llm_cool_docs/docs/llama\u5982\u4f55\u5b9e\u73b0\u7684\uff1f/gguf\u4f7f\u7528mmap\u8fdb\u884c\u9ad8\u6548\u8bfb\u53d6\u548c\u4fdd\u5b58"}}');var s=t(4848),r=t(8453);const i={sidebar_position:1},c="Tutorial Intro",l={},a=[];function d(e){const n={a:"a",h1:"h1",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"tutorial-intro",children:"Tutorial Intro"})}),"\n",(0,s.jsxs)(n.p,{children:["Let's discover ",(0,s.jsx)(n.strong,{children:"LLM"}),"."]}),"\n",(0,s.jsx)(n.h1,{id:"\u6846\u67b6\u7684\u57fa\u672c\u4ecb\u7ecd",children:"\u6846\u67b6\u7684\u57fa\u672c\u4ecb\u7ecd"}),"\n",(0,s.jsx)(n.p,{children:"\u5f53\u4eca\u6700\u706b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u6a21\u578b\u63a8\u7406\u90e8\u7f72\u6846\u67b6\uff0c\u4f60\u77e5\u9053\u6709\u54ea\u4e9b\u5417\uff1f\n\u4eceDeepSeek-V3\u6a21\u578b\u7684\u6a21\u578b\u4ecb\u7ecd\u9875\u9762\u5e94\u8be5\u53ef\u4ee5\u770b\u51fa\u4e00\u4e9b\u4e1c\u897f\u3002"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://raw.githubusercontent.com/KenForever1/CDN/main/deepseekv3.png",alt:""})}),"\n",(0,s.jsx)(n.p,{children:"\u53ef\u4ee5\u770b\u5230\u51e0\u4e2a\u9192\u76ee\u7684\u6846\u67b6\u5927\u540d\u3002"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://github.com/InternLM/lmdeploy/blob/main/README_zh-CN.md",children:"LMDeploy"}),"\uff1aLMDeploy \u7531 MMDeploy \u548c MMRazor \u56e2\u961f\u8054\u5408\u5f00\u53d1\uff0c\u662f\u6db5\u76d6\u4e86 LLM \u4efb\u52a1\u7684\u5168\u5957\u8f7b\u91cf\u5316\u3001\u90e8\u7f72\u548c\u670d\u52a1\u89e3\u51b3\u65b9\u6848\u3002"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"VLLM"}),": A high-throughput and memory-efficient inference and serving engine for LLMs."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://github.com/NVIDIA/TensorRT-LLM",children:"TensorRT-LLM"}),": \u82f1\u4f1f\u8fbe\u5f00\u6e90\u7684\u63a8\u7406\u5e93\uff0cTensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>c});var o=t(6540);const s={},r=o.createContext(s);function i(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);