"use strict";(self.webpackChunkllm_cool_docs=self.webpackChunkllm_cool_docs||[]).push([[685],{1283:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>i,contentTitle:()=>t,default:()=>c,frontMatter:()=>r,metadata:()=>l,toc:()=>o});const l=JSON.parse('{"id":"llama\u5982\u4f55\u5b9e\u73b0\u7684\uff1f/gguf\u4f7f\u7528mmap\u8fdb\u884c\u9ad8\u6548\u8bfb\u53d6\u548c\u4fdd\u5b58","title":"gguf\u4f7f\u7528mmap\u8fdb\u884c\u9ad8\u6548\u8bfb\u53d6\u548c\u4fdd\u5b58","description":"llama\u4e2d\u7684GGUF\u683c\u5f0f\u5982\u4f55\u52a0\u8f7d\u6a21\u578b\u6587\u4ef6\uff1f","source":"@site/docs/llama\u5982\u4f55\u5b9e\u73b0\u7684\uff1f/gguf\u4f7f\u7528mmap\u8fdb\u884c\u9ad8\u6548\u8bfb\u53d6\u548c\u4fdd\u5b58.md","sourceDirName":"llama\u5982\u4f55\u5b9e\u73b0\u7684\uff1f","slug":"/llama\u5982\u4f55\u5b9e\u73b0\u7684\uff1f/gguf\u4f7f\u7528mmap\u8fdb\u884c\u9ad8\u6548\u8bfb\u53d6\u548c\u4fdd\u5b58","permalink":"/llm_cool_docs/docs/llama\u5982\u4f55\u5b9e\u73b0\u7684\uff1f/gguf\u4f7f\u7528mmap\u8fdb\u884c\u9ad8\u6548\u8bfb\u53d6\u548c\u4fdd\u5b58","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/llama\u5982\u4f55\u5b9e\u73b0\u7684\uff1f/gguf\u4f7f\u7528mmap\u8fdb\u884c\u9ad8\u6548\u8bfb\u53d6\u548c\u4fdd\u5b58.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Tutorial Intro","permalink":"/llm_cool_docs/docs/intro"},"next":{"title":"Intervl2_8B\u6a21\u578b\u7684\u9884\u5904\u7406preprocess\u6709\u4f55\u4e0d\u540c","permalink":"/llm_cool_docs/docs/lmdeploy\u5982\u4f55\u5b9e\u73b0\u7684\uff1f/Intervl2_8B\u6a21\u578b\u7684\u9884\u5904\u7406preprocess\u6709\u4f55\u4e0d\u540c"}}');var s=n(4848),m=n(8453);const r={},t=void 0,i={},o=[{value:"llama\u4e2d\u7684GGUF\u683c\u5f0f\u5982\u4f55\u52a0\u8f7d\u6a21\u578b\u6587\u4ef6\uff1f",id:"llama\u4e2d\u7684gguf\u683c\u5f0f\u5982\u4f55\u52a0\u8f7d\u6a21\u578b\u6587\u4ef6",level:2},{value:"\u4e00\u8d77\u7aa5\u63a2\u6e90\u7801",id:"\u4e00\u8d77\u7aa5\u63a2\u6e90\u7801",level:2},{value:"llama\u83b7\u53d6tensor\u6570\u636e",id:"llama\u83b7\u53d6tensor\u6570\u636e",level:3},{value:"llama_mmap\u7c7b\u7684\u5b9e\u73b0",id:"llama_mmap\u7c7b\u7684\u5b9e\u73b0",level:3},{value:"llama_file\u7c7b\u7684\u5b9e\u73b0",id:"llama_file\u7c7b\u7684\u5b9e\u73b0",level:3},{value:"llama\u521d\u59cb\u5316mmap",id:"llama\u521d\u59cb\u5316mmap",level:3},{value:"\u518d\u6765\u804a\u4e00\u804amlock",id:"\u518d\u6765\u804a\u4e00\u804amlock",level:2},{value:"llama_mlock\u7c7b\u5982\u4f55\u5b9a\u4e49",id:"llama_mlock\u7c7b\u5982\u4f55\u5b9a\u4e49",level:3},{value:"grow_to\u51fd\u6570\u7684\u8c03\u7528\u4e4bload_all_data",id:"grow_to\u51fd\u6570\u7684\u8c03\u7528\u4e4bload_all_data",level:3},{value:"\u53c2\u8003",id:"\u53c2\u8003",level:2},{value:"\u4e0b\u4e00\u8bb2",id:"\u4e0b\u4e00\u8bb2",level:2}];function d(e){const a={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,m.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.h2,{id:"llama\u4e2d\u7684gguf\u683c\u5f0f\u5982\u4f55\u52a0\u8f7d\u6a21\u578b\u6587\u4ef6",children:"llama\u4e2d\u7684GGUF\u683c\u5f0f\u5982\u4f55\u52a0\u8f7d\u6a21\u578b\u6587\u4ef6\uff1f"}),"\n",(0,s.jsx)(a.p,{children:"gguf\u662fgg\u5927\u4f6c\u53d1\u660e\u7684\u4fdd\u5b58llm\u6a21\u578b\u7684\u683c\u5f0f\u3002\n\u4fdd\u5b58\u4e86header\u3001k-v\u3001tensor\uff0c\u652f\u6301\u591a\u79cd\u6a21\u578b\uff0c\u4fdd\u5b58GPT\u3001Phi3\u3001transformer\u7b49\u7b49\uff0c\u652f\u6301\u6269\u5c55\u3002\n\u5728gguf\u7ecf\u8fc7\u591a\u4e2a\u7248\u672c\u8fdb\u5316\u800c\u6765\uff0cggml\u3001GGJT\u3002\n\u4eceGGJT\u5f00\u59cb\u652f\u6301mmap\u3002\n\u6211\u4eec\u63a5\u4e0b\u6765\u8981\u804a\u4e00\u804a\u4e3a\u4ec0\u4e48\u8981\u652f\u6301mmap\u65b9\u5f0f\uff0c\u6709\u4ec0\u4e48\u4f5c\u7528\uff1f\ngguf\u5728\u4fdd\u5b58tensor\u6570\u636e\u65f6\u8fdb\u884c\u4e86align\u5bf9\u9f50\u64cd\u4f5c\uff0c\u4f7f\u7528mmap\u5c31\u53ef\u4ee5\u9ad8\u6548\u5feb\u901f\u7684\u52a0\u8f7d\u6570\u636e\u3002"}),"\n",(0,s.jsx)(a.p,{children:"\u4e0b\u9762\u4ecb\u7ecd\u7684\u6e90\u7801\u90fd\u51fa\u81eallama-cpp\u9879\u76ee\u4e2dllama.h\u548cllama.cpp\u3002"}),"\n",(0,s.jsx)(a.h2,{id:"\u4e00\u8d77\u7aa5\u63a2\u6e90\u7801",children:"\u4e00\u8d77\u7aa5\u63a2\u6e90\u7801"}),"\n",(0,s.jsx)(a.p,{children:"\u5173\u952e\u662falign\u5bf9\u9f50\uff0c\u7136\u540e\u65b9\u4fbf\u4f7f\u7528mmap\u9ad8\u6548load\u8bfb\u53d6\u6570\u636e\u3002\n\u5728llama_model_params\u4e2d\u5b9a\u4e49\u4e86use_mmap\uff0c\u53ef\u4ee5\u901a\u8fc7\u547d\u4ee4\u884c\u7b49\u63a7\u5236\uff0c\u662f\u5426\u8981\u4f7f\u7528mmap\u3002"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-cpp",children:"// llama.h\nstruct llama_model_params {\n    bool use_mmap          = true;  // use mmap for faster loads\n    bool use_mlock         = false; // use mlock to keep model in memory\n}\n"})}),"\n",(0,s.jsx)(a.p,{children:"\u5728llama\u7684\u5b9e\u73b0\u4e2d\uff0cmmap\u5728win\u5e73\u53f0\u548clinux\u5e73\u53f0\u5b9e\u73b0api\u662f\u4e0d\u540c\u7684\uff0c\u901a\u8fc7\u5b8f\u5b9a\u4e49\u4f7f\u7528\u4e0d\u540c\u7684\u4ee3\u7801\u6bb5\uff0c\u4e0b\u9762\u4ecb\u7ecdlinux\u5e73\u53f0\u5b9e\u73b0\uff0cwin\u53ef\u4ee5\u9605\u8bfb\u6e90\u7801\u3002"}),"\n",(0,s.jsx)(a.h3,{id:"llama\u83b7\u53d6tensor\u6570\u636e",children:"llama\u83b7\u53d6tensor\u6570\u636e"}),"\n",(0,s.jsx)(a.p,{children:"\u5728\u4e86\u89e3mmap\u4e4b\u524d\uff0c\u5148\u4ecellama\u7684\u4f7f\u7528\u4e0a\u9010\u6b65\u8fdb\u884c\u7aa5\u63a2\uff0c\u901a\u8fc7load_data_for\u51fd\u6570\u5bf9ggml_tensor\u6743\u91cd\u6570\u636e\u8fdb\u884c\u7684\u8bfb\u53d6\u3002\u53ef\u4ee5\u770b\u5230\u5206\u4e3a\u4e24\u79cd\uff0c\u4ecemmap\u8bfb\u53d6\uff0c\u4ecefile\u8bfb\u53d6\u3002"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-cpp",children:"// llama.cpp\nvoid load_data_for(struct ggml_tensor * cur) const {\n    const auto & w = require_weight(ggml_get_name(cur));\n\n    // \u5982\u679c\u4f7f\u7528mmap\uff0c\u76f4\u63a5\u4ecemmap\u83b7\u53d6\u6570\u636e\n    if (use_mmap) {\n        const auto & mapping = mappings.at(w.idx);\n        if (cur->data == nullptr) {\n            // \u5c06\u6307\u9488\u8d4b\u503c\u7ed9cur->data\n            cur->data = (uint8_t *)mapping->addr + w.offs;\n        } else {\n            // \u62f7\u8d1d\u6570\u636e\u5230cur->data\n            memcpy(cur->data, (uint8_t *)mapping->addr + w.offs, ggml_nbytes(cur));\n        }\n    } else {\n        // \u901a\u8fc7\u6587\u4ef6\u63cf\u8ff0\u7b26\u8bfb\u53d6\u6570\u636e\n        GGML_ASSERT(cur->data != nullptr);\n        GGML_ASSERT(w.idx < files.size());\n        const auto & file = files.at(w.idx);\n        file->seek(w.offs, SEEK_SET);\n        file->read_raw(cur->data, ggml_nbytes(cur));\n    }\n\n    if (check_tensors && !ggml_validate_row_data(cur->type, cur->data, ggml_nbytes(cur))) {\n        throw std::runtime_error(format(\"tensor '%s' has invalid data\", ggml_get_name(cur)));\n    }\n}\n"})}),"\n",(0,s.jsxs)(a.p,{children:["\u4e24\u79cd\u6570\u636e\u7ed3\u6784",(0,s.jsx)(a.strong,{children:"llama_mmap"}),"\u548c",(0,s.jsx)(a.strong,{children:"llama_file"}),"\uff0c\u5206\u522b\u5b9a\u4e49\u4e86mmap\u65b9\u5f0f\u548cfile\u8bfb\u53d6\u65b9\u5f0f\u3002"]}),"\n",(0,s.jsx)(a.h3,{id:"llama_mmap\u7c7b\u7684\u5b9e\u73b0",children:"llama_mmap\u7c7b\u7684\u5b9e\u73b0"}),"\n",(0,s.jsx)(a.p,{children:"\u5148\u770b\u770bllama_mmap, \u5728llama_mmap\u7684\u6784\u9020\u51fd\u6570\u4e2d\uff0c"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:["\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsx)(a.li,{children:"\u5148\u83b7\u53d6file\u7684fd\u6587\u4ef6\u63cf\u8ff0\u7b26"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(a.li,{children:["\n",(0,s.jsxs)(a.ol,{start:"2",children:["\n",(0,s.jsx)(a.li,{children:"\u8c03\u7528mmap\u51fd\u6570\uff0c\u83b7\u53d6\u5230\u6620\u5c04\u7684addr\uff0c\u4e00\u4e2allama_mmap\u5c31\u4fdd\u5b58\u4e86\u6620\u5c04\u6587\u4ef6\u540e\u7684\u5730\u5740\u548c\u6587\u4ef6\u7684size"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(a.li,{children:["\n",(0,s.jsxs)(a.ol,{start:"3",children:["\n",(0,s.jsx)(a.li,{children:"\u5c06\u6587\u4ef6size\u4fdd\u5b58\u5230mapped_fragments\u4e2d\n\u53ef\u4ee5\u770b\u5230\u9664\u4e86\u4e0a\u9762\u7684\u903b\u8f91\uff0c\u8fd8\u6709\u4e00\u4e9b\u8c03\u7528posix_fadvise\u51fd\u6570\u7684\u4f18\u5316\uff0c\u6bd4\u5982\u5efa\u8bae\u5185\u6838\u987a\u5e8f\u8bfb\u53d6\u6587\u4ef6\u3001\u9884\u53d6\u7b49\u3002"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-cpp",children:'struct llama_mmap {\n    void * addr;\n    size_t size;\n\n    // list of mapped fragments (first_offset, last_offset)\n    std::vector<std::pair<size_t, size_t>> mapped_fragments;\n\n    llama_mmap(struct llama_file * file, size_t prefetch = (size_t) -1 /* -1 = max value */, bool numa = false) {\n        size = file->size;\n        int fd = fileno(file->fp);\n        int flags = MAP_SHARED;\n        // prefetch/readahead impairs performance on NUMA systems\n        if (numa)  { prefetch = 0; }\n    #ifdef __linux__\n        // advise the kernel to read the file sequentially (increases readahead)\n        if (posix_fadvise(fd, 0, 0, POSIX_FADV_SEQUENTIAL)) {\n            LLAMA_LOG_WARN("warning: posix_fadvise(.., POSIX_FADV_SEQUENTIAL) failed: %s\\n",\n                    strerror(errno));\n        }\n        if (prefetch) { flags |= MAP_POPULATE; }\n    #endif\n        addr = mmap(NULL, file->size, PROT_READ, flags, fd, 0);\n        if (addr == MAP_FAILED) { // NOLINT\n            throw std::runtime_error(format("mmap failed: %s", strerror(errno)));\n        }\n\n        if (prefetch > 0) {\n            // advise the kernel to preload the mapped memory\n            if (posix_madvise(addr, std::min(file->size, prefetch), POSIX_MADV_WILLNEED)) {\n                LLAMA_LOG_WARN("warning: posix_madvise(.., POSIX_MADV_WILLNEED) failed: %s\\n",\n                        strerror(errno));\n            }\n        }\n        if (numa) {\n            // advise the kernel not to use readahead\n            // (because the next page might not belong on the same node)\n            if (posix_madvise(addr, file->size, POSIX_MADV_RANDOM)) {\n                LLAMA_LOG_WARN("warning: posix_madvise(.., POSIX_MADV_RANDOM) failed: %s\\n",\n                        strerror(errno));\n            }\n        }\n\n        // initialize list of mapped_fragments\n        mapped_fragments.emplace_back(0, file->size);\n    }\n}\nusing llama_mmaps = std::vector<std::unique_ptr<llama_mmap>>;\n'})}),"\n",(0,s.jsx)(a.h3,{id:"llama_file\u7c7b\u7684\u5b9e\u73b0",children:"llama_file\u7c7b\u7684\u5b9e\u73b0"}),"\n",(0,s.jsx)(a.p,{children:"llama_file\u7c7b\u4f7f\u7528fopen\u6253\u5f00\u4e86\u6587\u4ef6\uff0c\u5c01\u88c5\u4e86fread\u3001fwrite\u64cd\u4f5c\uff0c\u4fdd\u5b58\u4e86\u6587\u4ef6\u6d41\u6307\u9488\u548csize\u3002"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-cpp",children:'struct llama_file {\n    // use FILE * so we don\'t have to re-open the file to mmap\n    FILE * fp; // \u6587\u4ef6\u6d41\u6307\u9488\n    size_t size; // \u6587\u4ef6size\n\n    llama_file(const char * fname, const char * mode) {\n        // ggml_fopen\u5c01\u88c5\u4e86win32\u548clinux api\u7684\u533a\u522b\uff0c\u5728linux\u5b9e\u73b0\u8fd9\u91cc\u76f4\u63a5\u662ffopen(fname, mode)\n        fp = ggml_fopen(fname, mode);\n        if (fp == NULL) {\n            throw std::runtime_error(format("failed to open %s: %s", fname, strerror(errno)));\n        }\n        seek(0, SEEK_END);\n        size = tell();\n        seek(0, SEEK_SET);\n    }\n}\n'})}),"\n",(0,s.jsxs)(a.blockquote,{children:["\n",(0,s.jsx)(a.p,{children:"fopen\u548copen\u6700\u4e3b\u8981\u7684\u533a\u522b\u662ffopen\u5728\u7528\u6237\u6001\u4e0b\u5c31\u6709\u4e86\u7f13\u5b58\uff0c\u5728\u8fdb\u884cread\u548cwrite\u7684\u65f6\u5019\u51cf\u5c11\u4e86\u7528\u6237\u6001\u548c\u5185\u6838\u6001\u7684\u5207\u6362\uff0c\u800copen\u5219\u6bcf\u6b21\u90fd\u9700\u8981\u8fdb\u884c\u5185\u6838\u6001\u548c\u7528\u6237\u6001\u7684\u5207\u6362\uff1b\u8868\u73b0\u4e3a\uff0c\u5982\u679c\u987a\u5e8f\u8bbf\u95ee\u6587\u4ef6\uff0cfopen\u7cfb\u5217\u7684\u51fd\u6570\u8981\u6bd4\u76f4\u63a5\u8c03\u7528open\u7cfb\u5217\u5feb\uff1b\u5982\u679c\u968f\u673a\u8bbf\u95ee\u6587\u4ef6open\u8981\u6bd4fopen\u5feb\u3002fopen\u8fd4\u56de\u6587\u4ef6\u6d41\u800c\u4e0d\u662flinux\u4e0b\u6587\u4ef6\u53e5\u67c4\u3002"}),"\n"]}),"\n",(0,s.jsxs)(a.p,{children:["\u6240\u4ee5\uff0cllama\u5982\u679c\u4e0d\u4f7f\u7528mmap\u7684\u65b9\u5f0f\uff08use_mmap = false\u65f6\uff09\uff0c",(0,s.jsx)(a.strong,{children:"\u91c7\u7528file\u8bfb\u53d6\u6570\u636e\u5728linux\u4e0b\u662f\u4f7f\u7528fread\u3001fwrite api\u5b9e\u73b0\u7684"}),"\u3002"]}),"\n",(0,s.jsx)(a.h3,{id:"llama\u521d\u59cb\u5316mmap",children:"llama\u521d\u59cb\u5316mmap"}),"\n",(0,s.jsx)(a.p,{children:"\u6709\u4e86\u4e0a\u9762\u7684\u6570\u636e\u7ed3\u6784\u4ecb\u7ecd\uff0c\u6211\u4eec\u518d\u6765\u770b\u770bmmaping\u521d\u59cb\u5316\u64cd\u4f5c\u5c31\u5f88\u6e05\u6670\u4e86\u3002\u5c06\u6a21\u578b\u6587\u4ef6\u8fdb\u884cmmap\uff0c\u4fdd\u5b58mmap\u4ee5\u540e\u7684\u5730\u5740\u3002\u5728\u4e0a\u9762\u63d0\u5230\u7684load_data_for()\u51fd\u6570\u4e2d\u4f7f\u7528\uff0cllama_mmap\u4fdd\u5b58\u7684\u5730\u5740\u8bfb\u53d6\u6570\u636e\u3002"}),"\n",(0,s.jsx)(a.p,{children:"init_mappings\u51fd\u6570\u548cload_data_for\u51fd\u6570\u4e00\u6837\uff0c\u90fd\u5728llama_model_loader\u7c7b\u4e2d\u5b9a\u4e49\uff1a"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-cpp",children:"struct llama_model_loader {\n    bool use_mmap = false;\n\n    llama_files files;\n    llama_mmaps mappings;\n}\n"})}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-cpp",children:"// llama.cpp\nvoid init_mappings(bool prefetch = true, llama_mlocks * mlock_mmaps = nullptr) {\n    if (use_mmap) {\n        mappings.reserve(files.size());\n        mmaps_used.reserve(files.size());\n        for (const auto & file : files) {\n            std::unique_ptr<llama_mmap> mapping(new llama_mmap(file.get(), prefetch ? -1 : 0, ggml_is_numa()));\n            mmaps_used.emplace_back(mapping->size, 0);\n            if (mlock_mmaps) {\n                std::unique_ptr<llama_mlock> mlock_mmap(new llama_mlock());\n                mlock_mmap->init(mapping->addr);\n                mlock_mmaps->emplace_back(std::move(mlock_mmap));\n            }\n            mappings.emplace_back(std::move(mapping));\n        }\n    }\n\n    // compute the total size of all tensors for progress reporting\n    for (auto & w : weights) {\n        size_data += ggml_nbytes(w.tensor);\n    }\n}\n"})}),"\n",(0,s.jsx)(a.h2,{id:"\u518d\u6765\u804a\u4e00\u804amlock",children:"\u518d\u6765\u804a\u4e00\u804amlock"}),"\n",(0,s.jsx)(a.p,{children:"\u4e66\u63a5\u4e0a\u6587\uff0c\u5728llama_model_params\u4e2d\u5b9a\u4e49\u4e86use_mmap\uff0c\u53ef\u4ee5\u901a\u8fc7\u547d\u4ee4\u884c\u7b49\u63a7\u5236\uff0c\u662f\u5426\u8981\u4f7f\u7528mmap\u3002"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-cpp",children:"// llama.h\nstruct llama_model_params {\n    bool use_mmap          = true;  // use mmap for faster loads\n    bool use_mlock         = false; // use mlock to keep model in memory\n}\n"})}),"\n",(0,s.jsx)(a.p,{children:"\u5728\u524d\u9762\u7684llama_model_params\u53c2\u6570\u4e2d\u9664\u4e86\u63d0\u5230\u4e86use_mmap\u4ee5\u5916\uff0c\u8fd8\u6709\u4e00\u4e2a\u53c2\u6570use_mlock\u3002\u5b83\u7684\u610f\u601d\u662f\u5c06\u6a21\u578b\u7684\u5185\u5b58\u9501\u4f4f\uff0c\u907f\u514d\u56de\u6536\u3002\u4e5f\u5c31\u662f\u5c06\u6a21\u578b\u6587\u4ef6\u4e2d\u4fdd\u5b58\u7684tensors\u7684weight\u7559\u5728\u5185\u5b58\u4e2d\u3002"}),"\n",(0,s.jsx)(a.h3,{id:"llama_mlock\u7c7b\u5982\u4f55\u5b9a\u4e49",children:"llama_mlock\u7c7b\u5982\u4f55\u5b9a\u4e49"}),"\n",(0,s.jsx)(a.p,{children:"\u5728llama\u4e2d\u5b9a\u4e49\u4e3allama_mlock\u7ed3\u6784\u4f53\uff0c"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-cpp",children:"// Represents some region of memory being locked using mlock or VirtualLock;\n// will automatically unlock on destruction.\nstruct llama_mlock {\n    void * addr = NULL;\n    size_t size = 0;\n\n    ~llama_mlock() {\n        if (size) {\n            // \u8c03\u7528\u4e86munlock\u51fd\u6570\n            raw_unlock(addr, size);\n        }\n    }\n\n    void init(void * ptr) {\n        // NOLINT\u6ce8\u91ca\uff0c\u5982\u679c\u7f16\u7801\u8005\u786e\u8ba4\u6ca1\u95ee\u9898\uff0c\u662f\u8ba9\u4e00\u4e9b\u9759\u6001\u4ee3\u7801\u5206\u6790\u5de5\u5177\u4e0d\u62a5\u8b66\n        GGML_ASSERT(addr == NULL && size == 0); // NOLINT\n        addr = ptr;\n    }\n\n    // \u5728llama_load_all \u51fd\u6570\u4e2d\u8c03\u7528\n    void grow_to(size_t target_size) {\n        GGML_ASSERT(addr);\n        if (failed_already) {\n            return;\n        }\n        // \u83b7\u53d6pagesize\n        size_t granularity = lock_granularity();\n        // \u5c06target_size\u6309\u7167page_size\u5bf9\u9f50\uff0c\u8fd9\u662f\u4e00\u79cd\u5e38\u7528\u7684\u5199\u6cd5\uff0c\u6bd4\u5982\u5c06\u6570\u5b577\u6309\u71678\u5bf9\u9f50\uff0c\u5bf9\u9f50\u7ed3\u679c\u5c31\u662f8\n        target_size = (target_size + granularity - 1) & ~(granularity - 1);\n        if (target_size > size) {\n            // \u8c03\u7528mlock\n            if (raw_lock((uint8_t *) addr + size, target_size - size)) {\n                size = target_size;\n            } else {\n                failed_already = true;\n            }\n        }\n    }\n\n    bool raw_lock(const void * addr, size_t size) const {\n        if (!mlock(addr, size)) {\n            return true;\n        }\n        ...\n        // \u5982\u679c\u5185\u5b58\u4e0d\u8db3\uff0c\u901a\u8fc7ulimit -l\u8fdb\u884c\u67e5\u770b\uff0c\u901a\u8fc7ulimit\u8bbe\u7f6e\u66f4\u5927\u7684\u6570\u503c\n        return false;\n    }\n}\n"})}),"\n",(0,s.jsxs)(a.blockquote,{children:["\n",(0,s.jsx)(a.p,{children:"\u4e00\u822c\u7528\u6237\u7a7a\u95f4\u5173\u8054\u7684\u7269\u7406\u9875\u9762\u662f\u6309\u9700\u901a\u8fc7\u7f3a\u9875\u5f02\u5e38\u7684\u65b9\u5f0f\u5206\u914d\u548c\u8c03\u9875\uff0c\u5f53\u7cfb\u7edf\u7269\u7406\u5185\u5b58\u4e0d\u8db3\u65f6\u9875\u9762\u56de\u6536\u7b97\u6cd5\u4f1a\u56de\u6536\u4e00\u4e9b\u6700\u8fd1\u5f88\u5c11\u4f7f\u7528\u7684\u9875\u9762\uff0c\u4f46\u662f\u6709\u65f6\u5019\u6211\u4eec\u9700\u8981\u9501\u4f4f\u4e00\u4e9b\u7269\u7406\u9875\u9762\u9632\u6b62\u5176\u88ab\u56de\u6536\uff08\u5982\u65f6\u95f4\u6709\u4e25\u683c\u8981\u6c42\u7684\u5e94\u7528\uff09\uff0cLinux\u4e2d\u63d0\u4f9b\u4e86mlock\u76f8\u5173\u7684\u7cfb\u7edf\u8c03\u7528\u4f9b\u7528\u6237\u7a7a\u95f4\u4f7f\u7528\u6765\u9501\u4f4f\u90e8\u5206\u6216\u5168\u90e8\u7684\u5730\u5740\u7a7a\u95f4\u5173\u8054\u7684\u7269\u7406\u9875\u9762\u3002"}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"grow_to\u51fd\u6570\u7684\u8c03\u7528\u4e4bload_all_data",children:"grow_to\u51fd\u6570\u7684\u8c03\u7528\u4e4bload_all_data"}),"\n",(0,s.jsx)(a.p,{children:"llama\u63d0\u4f9b\u4e86grow_to\u51fd\u6570\uff0c\u5bf9target_size\u8fdb\u884cpagesize\u5bf9\u9f50\uff0c\u7136\u540emlock\u4f4f\u8fd9\u5757\u5185\u5b58\u3002\u90a3\u4e48\u5b83\u662f\u5728\u54ea\u513f\u4f7f\u7528\u7684\u5462\uff1f\n\u8fd9\u5c31\u548c\u4e0a\u9762\u7684use_map\u8054\u7cfb\u8d77\u6765\u4e86\uff0c\u5728load_all_data\u51fd\u6570\u5b9e\u73b0\u4e2d\u6709\u5982\u4e0b\u4ee3\u7801\uff0c\u5728\u4f7f\u7528use_mmap\u7684\u65f6\u5019\uff0c\u8c03\u7528\u4e86grow_to\u51fd\u6570\u3002"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-cpp",children:"struct llama_model_loader {\n    bool use_mmap = false;\n\n    llama_files files;\n    llama_mmaps mappings;\n\n    bool load_all_data(\n            struct ggml_context * ctx,\n            llama_buf_map & bufs_mmap,\n            llama_mlocks * lmlocks,\n            llama_progress_callback progress_callback,\n            void * progress_callback_user_data);\n\n    void init_mappings(bool prefetch = true, llama_mlocks * mlock_mmaps = nullptr);\n    \n    void load_data_for(struct ggml_tensor * cur);\n    \n}\n"})}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-cpp",children:"bool load_all_data(\n            struct ggml_context * ctx,\n            llama_buf_map & bufs_mmap,\n            llama_mlocks * lmlocks,\n            llama_progress_callback progress_callback,\n            void * progress_callback_user_data) {\n    // \u5faa\u73af\u5904\u7406\u6bcf\u4e00\u4e2atensor\n    for (struct ggml_tensor * cur = ggml_get_first_tensor(ctx); cur != NULL; cur = ggml_get_next_tensor(ctx, cur)) {\n        const auto * weight = get_weight(ggml_get_name(cur));\n            ......\n        if (use_mmap) {\n            // \u83b7\u53d6\u6bcf\u4e2atensor \u6743\u91cdweight\u7684mmaping\n            const auto & mapping = mappings.at(weight->idx);\n            ggml_backend_buffer_t buf_mmap = nullptr;\n            if (bufs_mmap.count(weight->idx)) {\n                buf_mmap = bufs_mmap.at(weight->idx);\n            }\n            // \u6839\u636eoffset\u5b9a\u4f4d\u5230\u6587\u4ef6\u4e2d\u6743\u91cd\u4fdd\u5b58\u7684\u4f4d\u7f6e\uff0c\u83b7\u53d6mmap data\u6307\u9488\n            uint8_t * data = (uint8_t *) mapping->addr + weight->offs;\n\n            if (check_tensors) {\n                validation_result.emplace_back(std::async(std::launch::async, [cur, data, n_size] {\n                    return std::make_pair(cur, ggml_validate_row_data(cur->type, data, n_size));\n                }));\n            }\n\n            GGML_ASSERT(buf_mmap || cur->data); // either we have a buffer to allocate the tensor in, or it is already allocated\n            if (buf_mmap && cur->data == nullptr) {\n                ggml_backend_tensor_alloc(buf_mmap, cur, data);\n                // \u9501\u4f4f\u4fdd\u5b58weight\u7684\u5185\u5b58\n                if (lmlocks) {\n                    const auto & lmlock = lmlocks->at(weight->idx);\n                    lmlock->grow_to(weight->offs + n_size);\n                }\n\n                auto & mmap_used = mmaps_used[weight->idx];\n                mmap_used.first  = std::min(mmap_used.first,  weight->offs);\n                mmap_used.second = std::max(mmap_used.second, weight->offs + n_size);\n            } else {\n                ggml_backend_tensor_set(cur, data, 0, n_size);\n            }\n        }\n    ....\n} \n"})}),"\n",(0,s.jsx)(a.h2,{id:"\u53c2\u8003",children:"\u53c2\u8003"}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.a,{href:"https://github.com/ggerganov/llama.cpp",children:"https://github.com/ggerganov/llama.cpp"}),"\n",(0,s.jsx)(a.a,{href:"https://github.com/ggerganov/ggml/discussions/492",children:"https://github.com/ggerganov/ggml/discussions/492"}),"\n",(0,s.jsx)(a.a,{href:"https://github.com/zylon-ai/private-gpt/issues/15",children:"https://github.com/zylon-ai/private-gpt/issues/15"})]}),"\n",(0,s.jsx)(a.h2,{id:"\u4e0b\u4e00\u8bb2",children:"\u4e0b\u4e00\u8bb2"}),"\n",(0,s.jsx)(a.p,{children:"\u5982\u4f55align\u5bf9\u9f50"})]})}function c(e={}){const{wrapper:a}={...(0,m.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,a,n)=>{n.d(a,{R:()=>r,x:()=>t});var l=n(6540);const s={},m=l.createContext(s);function r(e){const a=l.useContext(m);return l.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function t(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),l.createElement(m.Provider,{value:a},e.children)}}}]);